{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge-score py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement acceletate (from versions: none)\n",
      "ERROR: No matching distribution found for acceletate\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\Scripts\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\Scripts\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\Scripts\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\Scripts\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\Scripts\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\siddh\\onedrive\\desktop\\projects\\text_summarizer\\myenv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade acceletate\n",
    "%pip install -y transformers accelerate\n",
    "%pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset , load_metric\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#dowload & unzip data\n",
    "\n",
    "!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
    "!unzip summarizer-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to: dataset\\train.csv\n",
      "CSV file saved to: dataset\\test.csv\n",
      "CSV file saved to: dataset\\val.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# List of JSON files\n",
    "json_files = ['train.json', 'test.json', 'val.json']\n",
    "\n",
    "# Iterate through each JSON file\n",
    "for json_file in json_files:\n",
    "    # Construct the full path to the JSON file\n",
    "    json_file_path = os.path.join('dataset', json_file)\n",
    "\n",
    "    # Load JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_output_path = os.path.join('dataset', f'{json_file.split(\".\")[0]}.csv')\n",
    "    df.to_csv(csv_output_path, index=False)\n",
    "    print(f'CSV file saved to: {csv_output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "      <td>Sam: hey  overheard rick say something\\r\\nSam:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14727</th>\n",
       "      <td>13863028</td>\n",
       "      <td>Romeo is trying to get Greta to add him to her...</td>\n",
       "      <td>Romeo: You are on my ‘People you may know’ lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14728</th>\n",
       "      <td>13828570</td>\n",
       "      <td>Theresa is at work. She gets free food and fre...</td>\n",
       "      <td>Theresa: &lt;file_photo&gt;\\r\\nTheresa: &lt;file_photo&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14729</th>\n",
       "      <td>13819050</td>\n",
       "      <td>Japan is going to hunt whales again. Island an...</td>\n",
       "      <td>John: Every day some bad news. Japan will hunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14730</th>\n",
       "      <td>13828395</td>\n",
       "      <td>Celia couldn't make it to the afternoon with t...</td>\n",
       "      <td>Jennifer: Dear Celia! How are you doing?\\r\\nJe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14731</th>\n",
       "      <td>13729017</td>\n",
       "      <td>Georgia and Juliette are looking for a hotel i...</td>\n",
       "      <td>Georgia: are you ready for hotel hunting? We n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14732 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                            summary  \\\n",
       "0      13818513  Amanda baked cookies and will bring Jerry some...   \n",
       "1      13728867  Olivia and Olivier are voting for liberals in ...   \n",
       "2      13681000  Kim may try the pomodoro technique recommended...   \n",
       "3      13730747  Edward thinks he is in love with Bella. Rachel...   \n",
       "4      13728094  Sam is confused, because he overheard Rick com...   \n",
       "...         ...                                                ...   \n",
       "14727  13863028  Romeo is trying to get Greta to add him to her...   \n",
       "14728  13828570  Theresa is at work. She gets free food and fre...   \n",
       "14729  13819050  Japan is going to hunt whales again. Island an...   \n",
       "14730  13828395  Celia couldn't make it to the afternoon with t...   \n",
       "14731  13729017  Georgia and Juliette are looking for a hotel i...   \n",
       "\n",
       "                                                dialogue  \n",
       "0      Amanda: I baked  cookies. Do you want some?\\r\\...  \n",
       "1      Olivia: Who are you voting for in this electio...  \n",
       "2      Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...  \n",
       "3      Edward: Rachel, I think I'm in ove with Bella....  \n",
       "4      Sam: hey  overheard rick say something\\r\\nSam:...  \n",
       "...                                                  ...  \n",
       "14727  Romeo: You are on my ‘People you may know’ lis...  \n",
       "14728  Theresa: <file_photo>\\r\\nTheresa: <file_photo>...  \n",
       "14729  John: Every day some bad news. Japan will hunt...  \n",
       "14730  Jennifer: Dear Celia! How are you doing?\\r\\nJe...  \n",
       "14731  Georgia: are you ready for hotel hunting? We n...  \n",
       "\n",
       "[14732 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'dataset/train.csv'\n",
    "training_dataset = pd.read_csv(train_path)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Index(['id', 'summary', 'dialogue'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features: {training_dataset.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "   input_encodings = tokenizer(example_batch['dialogue'].tolist(), max_length= 1024, truncation= True )\n",
    "\n",
    "   with tokenizer.as_target_tokenizer():\n",
    "      target_encodings = tokenizer(example_batch['dialogue'].tolist(), max_length= 128, truncation= True )\n",
    "\n",
    "   return {\n",
    "      'input_ids': input_encodings['input_ids'],\n",
    "      'attention_mask': input_encodings['attention_mask'],\n",
    "      'target_ids': target_encodings['input_ids'],\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory dataset is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\summarizer.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/siddh/OneDrive/Desktop/Projects/Text_Summarizer/summarizer.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_samsum\u001b[39m=\u001b[39m load_from_disk(\u001b[39m'\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\datasets\\load.py:2252\u001b[0m, in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[0;32m   2250\u001b[0m     \u001b[39mreturn\u001b[39;00m DatasetDict\u001b[39m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory, storage_options\u001b[39m=\u001b[39mstorage_options)\n\u001b[0;32m   2251\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   2253\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m{\u001b[39;00mdataset_path\u001b[39m}\u001b[39;00m\u001b[39m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2254\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Directory dataset is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "dataset_samsum= load_from_disk('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\summarizer.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/siddh/OneDrive/Desktop/Projects/Text_Summarizer/summarizer.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_dataset_pt \u001b[39m=\u001b[39m training_dataset\u001b[39m.\u001b[39;49mmap(convert_examples_to_features)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:10123\u001b[0m, in \u001b[0;36mDataFrame.map\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m  10120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer\u001b[39m(x):\n\u001b[0;32m  10121\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39m_map_values(func, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m> 10123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(infer)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:10037\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10025\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10027\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m  10028\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10029\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10035\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m  10036\u001b[0m )\n\u001b[1;32m> 10037\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 963\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    965\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    978\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(v, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m    980\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    981\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    982\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    983\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:10121\u001b[0m, in \u001b[0;36mDataFrame.map.<locals>.infer\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m  10120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer\u001b[39m(x):\n\u001b[1;32m> 10121\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49m_map_values(func, na_action\u001b[39m=\u001b[39;49mna_action)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\myenv\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\OneDrive\\Desktop\\Projects\\Text_Summarizer\\summarizer.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siddh/OneDrive/Desktop/Projects/Text_Summarizer/summarizer.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_examples_to_features\u001b[39m(example_batch):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/siddh/OneDrive/Desktop/Projects/Text_Summarizer/summarizer.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m    input_encodings \u001b[39m=\u001b[39m tokenizer(example_batch[\u001b[39m'\u001b[39;49m\u001b[39mdialogue\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mtolist(), max_length\u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m, truncation\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siddh/OneDrive/Desktop/Projects/Text_Summarizer/summarizer.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m    \u001b[39mwith\u001b[39;00m tokenizer\u001b[39m.\u001b[39mas_target_tokenizer():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siddh/OneDrive/Desktop/Projects/Text_Summarizer/summarizer.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       target_encodings \u001b[39m=\u001b[39m tokenizer(example_batch[\u001b[39m'\u001b[39m\u001b[39mdialogue\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(), max_length\u001b[39m=\u001b[39m \u001b[39m128\u001b[39m, truncation\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m )\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "training_dataset_pt = training_dataset.map(convert_examples_to_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
